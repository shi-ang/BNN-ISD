import numpy as np
import pandas as pd
from sklearn.utils import shuffle as sklearn_shuffle
from sklearn.utils import check_random_state
from sklearn.datasets import make_friedman1


def make_data(
        dataset: str
) -> (pd.DataFrame, any):
    if dataset == "Synthetic-I":
        return make_synthetic_data()
    elif dataset == "Synthetic-II":
        return generate_synthetic_data(type='poly')
    elif dataset == "SUPPORT":
        return make_support_data(), None
    elif dataset == "NACD":
        return make_nacd_data(), None
    elif dataset == "MIMIC":
        return make_mimic_data(), None
    else:
        raise ValueError("Dataset name not recognized.")


def make_synthetic_data(
        n_samples: int = 10000,
        n_noise_features: int = 47,
        base_hazard: int = 0.1,
        percent_censor: float = 0.3
) -> (pd.DataFrame, np.ndarray):
    """Generates a synthetic survival dataset with linear hazard. (Borrowed form torchmtlr)"""
    x = np.random.standard_normal((n_samples, n_noise_features + 3))
    hazards = x[:, 0] + 2 * x[:, 1] - 0.5 * x[:, 2]
    event_time = np.random.exponential(1 / (base_hazard * np.exp(hazards)))
    censor_time = np.quantile(event_time, 1 - percent_censor)

    time = np.minimum(event_time, censor_time)
    event = (event_time < censor_time).astype(np.int)

    df = pd.DataFrame({
        "time": time,
        "event": event,
        "true_time": event_time,
        **{f"x{i+1}": x[:, i] for i in range(x.shape[1])}
    })
    return df, np.array([1, 2, -0.5])


def generate_synthetic_data(
        censor_dist: str = 'Uniform',
        n_samples: int = 10000,
        n_features: int = 10,
        type: str = 'linear'
) -> (pd.DataFrame, np.ndarray):
    if type == "linear":
        X, true_times, coef = make_regression(n_samples=n_samples, n_features=n_features, n_informative=5,
                                              bias=0, noise=0.05, random_state=None)
    elif type == "poly":
        X, true_times = make_friedman1(n_samples, n_features=n_features, noise=0.05)
        coef = None
    else:
        raise NotImplementedError

    true_times = true_times.round(decimals=1)
    X = X.round(decimals=1)
    # make sure the dataset is positive
    if true_times.min() < 0:
        true_times += -true_times.min() + 0.1
    times = np.copy(true_times)

    if censor_dist == "Uniform":
        event_status = np.ones(n_samples)
        censor_time = np.random.uniform(low=true_times.min(), high=true_times.max(), size=n_samples).round(decimals=1)

        event_status[censor_time < true_times] = 0
        times[event_status == 0] = censor_time[event_status == 0]
        df = pd.DataFrame({
            "time": times,
            "event": event_status,
            "true_time": true_times,
            **{f"x{i+1}": X[:, i] for i in range(X.shape[1])}
        })
        return df, coef
    else:
        raise NotImplementedError


def make_regression(
        n_samples=10000,
        n_features=100,
        n_informative=20,
        bias=0.0,
        noise=0.05,
        shuffle=True,
        random_state=None
) -> (np.ndarray, np.ndarray, np.ndarray):
    """Generate a random regression problem. (Borrowed from sklearn)

    The input set can either be well conditioned (by default) or have a low
    rank-fat tail singular profile. See :func:`make_low_rank_matrix` for
    more details.

    The output is generated by applying a (potentially biased) random linear
    regression model with `n_informative` nonzero regressors to the previously
    generated input and some gaussian centered noise with some adjustable
    scale.

    Read more in the :ref:`User Guide <sample_generators>`.

    Parameters
    ----------
    n_samples : int, default=100
        The number of samples.

    n_features : int, default=100
        The number of features.

    n_informative : int, default=10
        The number of informative features, i.e., the number of features used
        to build the linear model used to generate the output.

    bias : float, default=0.0
        The bias term in the underlying linear model.

    noise : float, default=0.0
        The standard deviation of the gaussian noise applied to the output.

    shuffle : bool, default=True
        Shuffle the samples and the features.

    random_state : int, RandomState instance or None, default=None
        Determines random number generation for dataset creation. Pass an int
        for reproducible output across multiple function calls.
        See :term:`Glossary <random_state>`.

    Returns
    -------
    X : ndarray of shape (n_samples, n_features)
        The input samples.

    y : ndarray of shape (n_samples,) or (n_samples, n_targets)
        The output values.

    coef : ndarray of shape (n_features,) or (n_features, n_targets)
        The coefficient of the underlying linear model. It is returned only if
        coef is True.
    """
    generator = check_random_state(random_state)

    # Randomly generate a well conditioned input set
    X = generator.randn(n_samples, n_features)

    # Generate a ground truth model with only n_informative features being non
    # zeros (the other features are not correlated to y and should be ignored
    # by a sparsifying regularizers such as L1 or elastic net)
    ground_truth = np.zeros((n_features, 1))
    ground_truth[:n_informative, :] = 2.5 * generator.rand(n_informative, 1)

    y = np.dot(X, ground_truth) + bias

    # Add noise
    assert noise > 0.0, "The standard deviation of the noise must higher than 0"
    y += generator.normal(loc=0.0, scale=noise, size=y.shape)

    # Randomly permute samples and features
    if shuffle:
        X, y = sklearn_shuffle(X, y, random_state=generator)

        indices = np.arange(n_features)
        generator.shuffle(indices)
        X[:, :] = X[:, indices]
        ground_truth = ground_truth[indices]

    y = np.squeeze(y)
    return X, y, np.squeeze(ground_truth)


def make_support_data():
    """Downloads and preprocesses the SUPPORT dataset from [1]_.

    The missing values are filled using either the recommended
    standard values, the mean (for continuous variables) or the mode
    (for categorical variables).
    Refer to the dataset description at
    https://biostat.app.vumc.org/wiki/Main/SupportDesc for more information.

    Returns
    -------
    pd.DataFrame
        DataFrame with processed covariates for one patient in each row.

    References
    ----------
    ..[1] W. A. Knaus et al., ‘The SUPPORT Prognostic Model: Objective Estimates of Survival
    for Seriously Ill Hospitalized Adults’, Ann Intern Med, vol. 122, no. 3, p. 191, Feb. 1995.
    """
    url = "https://biostat.app.vumc.org/wiki/pub/Main/DataSets/support2csv.zip"

    # Remove other target columns and other model predictions
    cols_to_drop = [
        "hospdead",
        "slos",
        "charges",
        "totcst",
        "totmcst",
        "avtisst",
        "sfdm2",
        "adlp",     # "adlp", "adls", and "dzgroup" were used in other preprocessing steps,
        # see https://github.com/autonlab/auton-survival/blob/master/auton_survival/datasets.py
        "adls",
        "dzgroup",
        "sps",
        "aps",
        "surv2m",
        "surv6m",
        "prg2m",
        "prg6m",
        "dnr",
        "dnrday",
        "hday",
    ]

    # `death` is the overall survival event indicator
    # `d.time` is the time to death from any cause or censoring
    data = (pd.read_csv(url)
            .drop(cols_to_drop, axis=1)
            .rename(columns={"d.time": "time", "death": "event"}))
    data["event"] = data["event"].astype(int)

    data["ca"] = (data["ca"] == "metastatic").astype(int)

    # use recommended default values from official dataset description ()
    # or mean (for continuous variables)/mode (for categorical variables) if not given
    fill_vals = {
        "alb": 3.5,
        "pafi": 333.3,
        "bili": 1.01,
        "crea": 1.01,
        "bun": 6.51,
        "wblc": 9,
        "urine": 2502,
        "edu": data["edu"].mean(),
        "ph": data["ph"].mean(),
        "glucose": data["glucose"].mean(),
        "scoma": data["scoma"].mean(),
        "meanbp": data["meanbp"].mean(),
        "hrt": data["hrt"].mean(),
        "resp": data["resp"].mean(),
        "temp": data["temp"].mean(),
        "sod": data["sod"].mean(),
        "income": data["income"].mode()[0],
        "race": data["race"].mode()[0],
    }
    data = data.fillna(fill_vals)

    data.sex.replace({'male': 1, 'female': 0}, inplace=True)
    data.income.replace({'under $11k': 0, '$11-$25k': 1, '$25-$50k': 2, '>$50k': 3}, inplace=True)
    skip_cols = ['event', 'sex', 'time', 'dzclass', 'race', 'diabetes', 'dementia', 'ca']
    cols_standardize = list(set(data.columns.to_list()).symmetric_difference(skip_cols))
    data[cols_standardize] = data[cols_standardize].apply(lambda x: (x - x.mean()) / x.std())

    # one-hot encode categorical variables
    onehot_cols = ["dzclass", "race"]
    data = pd.get_dummies(data, columns=onehot_cols, drop_first=True)
    data = data.rename(columns={"dzclass_COPD/CHF/Cirrhosis": "dzclass_COPD"})

    return data


def make_nacd_data() -> pd.DataFrame:
    cols_to_drop = ['PERFORMANCE_STATUS', 'STAGE_NUMERICAL', 'AGE65']
    data = pd.read_csv("data/NACD/NACD_Full.csv").drop(cols_to_drop, axis=1).rename(columns={"delta": "event"})

    cols_standardize = ['BOX1_SCORE', 'BOX2_SCORE', 'BOX3_SCORE', 'BMI', 'WEIGHT_CHANGEPOINT',
                        'AGE', 'GRANULOCYTES', 'LDH_SERUM', 'LYMPHOCYTES',
                        'PLATELET', 'WBC_COUNT', 'CALCIUM_SERUM', 'HGB', 'CREATININE_SERUM', 'ALBUMIN']
    data[cols_standardize] = data[cols_standardize].apply(lambda x: (x - x.mean()) / x.std())
    return data


def make_mimic_data() -> pd.DataFrame:
    data = pd.read_csv('data/MIMIC/MIMIC_IV_v2.0_preprocessed.csv')
    skip_cols = ['event', 'is_male', 'time', 'is_white', 'renal', 'cns', 'coagulation', 'cardiovascular']
    cols_standardize = list(set(data.columns.to_list()).symmetric_difference(skip_cols))
    data[cols_standardize] = data[cols_standardize].apply(lambda x: (x - x.mean()) / x.std())
    return data
